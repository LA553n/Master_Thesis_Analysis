{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lucas J. Assen\n",
    "### 25-03-2025\n",
    "# Section A: Transform data to the FIF format\n",
    "## Loading the data in the bdf format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# identify the folders where the data is stored, change the paths to your own\n",
    "buddhist = Path(\"/Users/lucas.assen/Desktop/TempData/Buddhist\")\n",
    "control = Path(\"/Users/lucas.assen/Desktop/TempData/Controls\")\n",
    "\n",
    "# import the files for each condition\n",
    "buddhist_files = sorted(glob.glob(str(buddhist / \"*bdf\")))\n",
    "control_files = sorted(glob.glob(str(control / \"*bdf\")))\n",
    "\n",
    "#defining a dictionanry and function for loading and storing the data\n",
    "D_0 = {\"Buddhists\": {}, \"Controls\": {}}\n",
    "\n",
    "def load_data_files(file_list, condition):\n",
    "    for file in file_list:\n",
    "        Participant_ID = Path(file).stem\n",
    "        print(f\"Loading {Participant_ID} ({condition})...\")\n",
    "        raw = mne.io.read_raw_bdf(file, preload=True)\n",
    "        D_0[condition][Participant_ID] = raw\n",
    "\n",
    "load_data_files(buddhist_files, \"Buddhists\")\n",
    "load_data_files(control_files, \"Controls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping the events within the data to the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One of the buddhists participants is set as the raw since, as oposed to the controls, the buddhist group stim channel contains event triggers \n",
    "# for both the meditation types.\n",
    "raw = D_0[\"Buddhists\"][\"3\"]\n",
    "events = mne.find_events(raw, stim_channel=\"Status\")\n",
    "print(events)\n",
    "\n",
    "event_mappings = {1: \"rating_poor\", \n",
    "                  2: \"rating_reasonable\",\n",
    "                  3: \"rating_excellent\",\n",
    "                  10: \"Announcement_FA_practice\",\n",
    "                  20: \"FA_practice_start_signal\",\n",
    "                  30: \"FA_practice_stop_signal\",\n",
    "                  40: \"FA_rating_signal\",\n",
    "                  11: \"Announcement_LK_practice\",\n",
    "                  21: \"LK_practice_start_signal\",\n",
    "                  31: \"LK_practice_stop_signal\",\n",
    "                  41: \"LK_rating_signal\",\n",
    "                  99: \"Part_FA_start_indicator\",\n",
    "                  100: \"Part_LK_start_indicator\",\n",
    "                  199: \"Part_FA_stop_indicator\",\n",
    "                  200: \"Part_LK_stop_indicator\",\n",
    "                  101: \"Eyes_open_signal\",\n",
    "                  102: \"Eyes_closed_signal\"}\n",
    "# In this loop an iteration takes places over both conditions across all participants. It takes into account the event present in each individual\n",
    "# participants' stimulus channel. Then only the event mappings relevant to the individual participant are annotated.\n",
    "\n",
    "for condition in D_0.keys(): \n",
    "    for participant_id, raw in D_0[condition].items():\n",
    "        print(f\"Processing participant {participant_id} ({condition})\")\n",
    "\n",
    "        # Extract events from the current participant that is being iterated over.\n",
    "        events = mne.find_events(raw, stim_channel=\"Status\")\n",
    "        unique_event_codes = np.unique(events[:, 2]) \n",
    "        filtered_event_mappings = {code: event_mappings[code] for code in unique_event_codes if code in event_mappings}\n",
    "        print(f\"Filtered event mappings for {participant_id}: {filtered_event_mappings}\")\n",
    "\n",
    "        # Convert events to annotations\n",
    "        annot_from_events = mne.annotations_from_events(events=events,\n",
    "                                                        event_desc=filtered_event_mappings,  \n",
    "                                                        sfreq=raw.info[\"sfreq\"],\n",
    "                                                        orig_time=raw.info[\"meas_date\"],)\n",
    "\n",
    "        # Apply annotations to the raw data of the participant\n",
    "        raw.set_annotations(annot_from_events)\n",
    "        print(f\"Annotations applied to {participant_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperating the EOG channels from the EEG channels in the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As the EOG channels are not specified in the data, and are included as EEG channels, the following code is used to specify \n",
    "# the EOG channels as EOG channels.\n",
    "eog_channels = ['LH', 'RH', 'LV', 'UV']\n",
    "\n",
    "# The following iterates over both conditions to set the channel types for the EOG channels.\n",
    "for condition in [\"Buddhists\", \"Controls\"]:\n",
    "    for participant_id, raw in D_0[condition].items():\n",
    "        raw.set_channel_types({ch: 'eog' for ch in eog_channels})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attaching the digitisation points for the biosemi64 to the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the bdf data format digitisation points are not automatically saved in the data. As a 10-20 biosemi64 set-up was used, the points are \n",
    "# added based on that. For the Mastiods (M1 and M2) and the Nasion points (Nz and Snz) the points are manually defined as these channels are \n",
    "# non standard additions to the biosemi64 set-up.\n",
    "\n",
    "#The locations of the missing electrodes are defined. The locations are based on typical spots \n",
    "#for the mastoid(M1/2) and Nasion (Nz/SNz) points\n",
    "custom_electrodes = {\"M1\": np.array([-0.08, -0.04, 0.0]),\n",
    "                     \"M2\": np.array([0.08, -0.04, 0.0]),\n",
    "                     \"Nz\": np.array([0.0, 0.10, 0.0]),\n",
    "                     \"SNz\": np.array([0.0, 0.12, 0.0])}\n",
    "\n",
    "# Load the standard Biosemi64 montage\n",
    "biosemi_montage = mne.channels.make_standard_montage('biosemi64')\n",
    "biosemi_digitisation = biosemi_montage._get_ch_pos()\n",
    "\n",
    "# Here the standard positions are updated to include the manually defined additional electrodes\n",
    "biosemi_digitisation.update(custom_electrodes)\n",
    "updated_montage = mne.channels.make_dig_montage(ch_pos=biosemi_digitisation, coord_frame=\"head\")\n",
    "\n",
    "# Iterate over both groups and participants to apply the digitisation points.\n",
    "for condition in [\"Buddhists\", \"Controls\"]:\n",
    "    for participant_id, raw in D_0[condition].items():\n",
    "        raw.set_montage(updated_montage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An initial filter is applied to the data with a highpass of 0.1 Hz and a lowpass of 40 Hz.\n",
    "Initial_filter_dict = {}\n",
    "\n",
    "for condition in D_0.keys():\n",
    "    Initial_filter_dict[condition] = {} \n",
    "    for participant_id, raw in D_0[condition].items():\n",
    "        print(f\"Filtering participant {participant_id} in condition {condition}...\")\n",
    "        Initial_filter = raw.copy().filter(l_freq=0.1, h_freq=40, fir_design='firwin', phase='zero-double')\n",
    "        Initial_filter_dict[condition][participant_id] = Initial_filter\n",
    "print(\"Initial filtering complete for all participants.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming and saving the data file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is transformed and saved to a fif format as it is a more useful format for EEG data processing in MNE.\n",
    "# Separate output directories for Buddhists and Controls, be sure to change the directory paths to your own.\n",
    "buddhist_output_dir = \"/Users/lucas.assen/Desktop/Master Thesis/Data/Buddhist\"  \n",
    "control_output_dir = \"/Users/lucas.assen/Desktop/Master Thesis/Data/Controls\"\n",
    "\n",
    "os.makedirs(buddhist_output_dir, exist_ok=True)\n",
    "os.makedirs(control_output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over both groups and participants in the dictionary\n",
    "for condition in [\"Buddhists\", \"Controls\"]:\n",
    "    for participant_id, raw in Initial_filter_dict[condition].items():\n",
    "        if condition == \"Buddhists\":\n",
    "            output_file = os.path.join(buddhist_output_dir, f\"{participant_id}.fif\")\n",
    "        else:\n",
    "            output_file = os.path.join(control_output_dir, f\"{participant_id}.fif\")\n",
    "        raw.save(output_file, overwrite=True)\n",
    "        print(f\"Saved {participant_id} ({condition}) as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B: Pre-processing\n",
    "## Loading back in the FIF transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import os\n",
    "from mne.preprocessing import ICA, corrmap, create_ecg_epochs, create_eog_epochs\n",
    "from mne import pick_types\n",
    "from mne.preprocessing import EOGRegression\n",
    "\n",
    "# Identify the folders where the data is stored, change to your own directory paths\n",
    "buddhist = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Data/Buddhist\")\n",
    "control = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Data/Controls\")\n",
    "\n",
    "# Load the converted FIF files\n",
    "buddhist_fif_files = sorted(glob.glob(str(buddhist / \"*.fif\")))\n",
    "control_fif_files = sorted(glob.glob(str(control / \"*.fif\")))\n",
    "\n",
    "D_0 = {\"Buddhists\": {}, \"Controls\": {}}\n",
    "\n",
    "def load_data_files(file_list, condition):\n",
    "    for file in file_list:\n",
    "        Participant_ID = Path(file).stem\n",
    "        print(f\"Loading {Participant_ID} ({condition})...\")\n",
    "\n",
    "        raw = mne.io.read_raw_fif(file, preload=True)\n",
    "        D_0[condition][Participant_ID] = raw\n",
    "\n",
    "load_data_files(buddhist_fif_files, \"Buddhists\")\n",
    "load_data_files(control_fif_files, \"Controls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rereferencing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The data is re-referenced to the average of the TP7 and TP8 channels for all participants.\n",
    "for condition in D_0.keys():  \n",
    "    for participant_id, raw in D_0[condition].items():\n",
    "        raw.set_eeg_reference(ref_channels=[\"TP7\", \"TP8\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter prior to the ICA per participant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A pre ICA filtering is applied to the raws of each participant. These are done on copies of the raw and are stored in a new dictionary.\n",
    "# Here a highpass of 1 Hz is applied to remove slow drifts at low frequencies which improves the ICA performance.\n",
    "Initial_filter_dict = {}\n",
    "\n",
    "for condition in D_0.keys():\n",
    "    Initial_filter_dict[condition] = {} \n",
    "    for participant_id, raw in D_0[condition].items():\n",
    "        print(f\"Filtering participant {participant_id} in condition {condition}...\")\n",
    "        Initial_filter = raw.copy().filter(l_freq=1, h_freq=40, fir_design='firwin', phase='zero-double')\n",
    "        Initial_filter_dict[condition][participant_id] = Initial_filter\n",
    "print(\"Initial filtering complete for all participants.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the ICA components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First select for which participant the ICA is run by changing the participant that is picked from the dictionary as Filt_ICA.\n",
    "# Then ICA is ran. The number of initial components is set as 15 and a random state of 1 is picked to ensure that the same outcome is generated each time.\n",
    "# Lastly the ICA is fitted.\n",
    "Filt_ICA = Initial_filter_dict[\"Buddhists\"][\"1\"]    \n",
    "ica = ICA(n_components=15, max_iter=\"auto\", random_state=1)\n",
    "ica.fit(Filt_ICA)\n",
    "ica\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This first part shows the overal explained variance of all ICA components combined\n",
    "explained_var_ica = ica.get_explained_variance_ratio(Filt_ICA)\n",
    "for channel_type, ratio in explained_var_ica.items():\n",
    "    print(f\"Fraction of {channel_type} variance explained by all components: {ratio}\")\n",
    "\n",
    "# Here you can select a specific component from 0-14 to see its explained variance\n",
    "explained_var_per_comp = ica.get_explained_variance_ratio(Filt_ICA, components=[0], ch_type=\"eeg\")\n",
    "ratio_percent = round(100 * explained_var_per_comp [\"eeg\"])\n",
    "print(f\"Fraction of variance in EEG signal explained by first component: \"\n",
    "      f\"{ratio_percent}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting the Components to be removed and applying the ICA to the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, select the participants for which you calculated the ICA. A topography and standard plot of the components is created. Furthermore, you can pick specific components \n",
    "# to see their properties by specifying the picks.\n",
    "raw = D_0[\"Buddhists\"][\"1\"]\n",
    "ica.plot_sources(raw, show_scrollbars=False)\n",
    "ica.plot_components()\n",
    "ica.plot_properties(raw, picks=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After visial inspection of the components, the compents that should be excluded are selected\n",
    "ica.exclude = [0, 7, 10, 12, 14]\n",
    "\n",
    "# The excluded components are applied to the raw data and plotted to a copy to see the difference after removing the ICA\n",
    "artifact_picks = mne.pick_types(raw.info, meg=False, eeg=True, exclude=\"bads\")\n",
    "raw.copy().plot()\n",
    "ica.apply(raw)\n",
    "raw.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The ICA cleaned version of the data is saved to a new folder.\n",
    "# Make sure to use the right output_file depending on whether you are working with the Buddhist or Control group, and ensure the output directories is your own.\n",
    "participant_id = raw.info.get('subject_info', {}).get('id', 'unknown')\n",
    "buddhist_output_dir = \"/Users/lucas.assen/Desktop/Master Thesis/Buddhists_ICA_clean\"  \n",
    "control_output_dir = \"/Users/lucas.assen/Desktop/Master Thesis/Controls_ICA_clean\"\n",
    "#output_file = os.path.join(buddhist_output_dir, f\"{participant_id}_ICA_cleaned.fif\")\n",
    "output_file = os.path.join(control_output_dir, f\"{participant_id}_ICA_cleaned.fif\")\n",
    "raw.save(output_file, overwrite=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section three: Final Filtering\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the final filtering the data ica cleaned data is loaded. Change the directory paths to your own.\n",
    "\n",
    "import mne\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import os\n",
    "from mne.preprocessing import ICA, corrmap, create_ecg_epochs, create_eog_epochs\n",
    "from mne import pick_types\n",
    "from mne.preprocessing import EOGRegression\n",
    "\n",
    "# Identify the folders where the data is stored\n",
    "buddhist = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Buddhists_ICA_clean\")\n",
    "control = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Controls_ICA_clean\")\n",
    "\n",
    "# Load the converted FIF files\n",
    "buddhist_fif_files = sorted(glob.glob(str(buddhist / \"*.fif\")))\n",
    "control_fif_files = sorted(glob.glob(str(control / \"*.fif\")))\n",
    "\n",
    "D_0 = {\"Buddhists\": {}, \"Controls\": {}}\n",
    "\n",
    "def load_data_files(file_list, condition):\n",
    "    for file in file_list:\n",
    "        Participant_ID = Path(file).stem\n",
    "        print(f\"Loading {Participant_ID} ({condition})\")\n",
    "\n",
    "        raw = mne.io.read_raw_fif(file, preload=True)\n",
    "        D_0[condition][Participant_ID] = raw\n",
    "\n",
    "load_data_files(buddhist_fif_files, \"Buddhists\")\n",
    "load_data_files(control_fif_files, \"Controls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying the final filter and saving the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A filtering is applied to the raws of each participant specifying the filter to a high-pass of 0.3 Hz and a low-pass of 35 Hz. \n",
    "# These are done on copies of the ica cleaned raws and stored in a new dictionary. Change directory paths to your own.\n",
    "Preprocessed_dict = {}\n",
    "buddhist_output_dir = \"/Users/lucas.assen/Desktop/Master Thesis/Buddhists_preprocessed\"  \n",
    "control_output_dir = \"/Users/lucas.assen/Desktop/Master Thesis/Controls_preprocessed\"\n",
    "\n",
    "for condition in D_0.keys():\n",
    "    Preprocessed_dict[condition] = {}  # Initialize a sub-dictionary for each condition\n",
    "    for participant_id, raw in D_0[condition].items():\n",
    "        print(f\"Filtering participant {participant_id} in condition {condition}\")\n",
    "        Initial_filter = raw.copy().filter(l_freq=0.3, h_freq=35, fir_design='firwin', phase='zero-double')\n",
    "        Preprocessed_dict[condition][participant_id] = Initial_filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the final filtering the ICA cleaned and preprocessed data is saved to a new folder.\n",
    "for condition in [\"Buddhists\", \"Controls\"]:\n",
    "    for participant_id, raw in Preprocessed_dict[condition].items():\n",
    "        if condition == \"Buddhists\":\n",
    "            output_file = os.path.join(buddhist_output_dir, f\"{participant_id}_processed.fif\")\n",
    "        else:\n",
    "            output_file = os.path.join(control_output_dir, f\"{participant_id}_processed.fif\")\n",
    "        raw.save(output_file, overwrite=True)\n",
    "        print(f\"Saved {participant_id} ({condition}) as {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section four: Improved bad channel detection\n",
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The PREP pipeline bad channel detection methods are used to identify any remaining bad channels in the preprocessed data.\n",
    "# First the data is loaded from the preprocessed folders.\n",
    "\n",
    "import mne \n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pyvista as pv\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "from scipy import interpolate\n",
    "import matplotlib.pyplot as plt\n",
    "import os, warnings, numpy as np, pandas as pd, mne\n",
    "from pyprep import NoisyChannels\n",
    "\n",
    "# Identify the folders where the data is stored, change the paths to your own.\n",
    "buddhist = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Buddhists_preprocessed\")\n",
    "control = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Controls_preprocessed\")\n",
    "\n",
    "# Load the converted FIF files\n",
    "buddhist_fif_files = sorted(glob.glob(str(buddhist / \"*.fif\")))\n",
    "control_fif_files = sorted(glob.glob(str(control / \"*.fif\")))\n",
    "\n",
    "D_0 = {\"Buddhists\": {}, \"Controls\": {}}\n",
    "\n",
    "def load_data_files(file_list, condition):\n",
    "    for file in file_list:\n",
    "        Participant_ID = Path(file).stem\n",
    "        print(f\"Loading {Participant_ID} ({condition})\")\n",
    "\n",
    "        raw = mne.io.read_raw_fif(file, preload=True)\n",
    "        D_0[condition][Participant_ID] = raw\n",
    "\n",
    "load_data_files(buddhist_fif_files, \"Buddhists\")\n",
    "load_data_files(control_fif_files, \"Controls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the noisy channel detection methods from the PREP pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The HP_TEMP variable is used to set the frequency of the high-pass filter applied to the data before running the PREP pipeline. \n",
    "#It is again advised to set it at one to remove slow drifts\n",
    "#The output is a csv file containing the channels that are considered bad according to each metric included in the PREP pipeline. Change the path to your own \n",
    "#This can be subjected to visual inspection and ultimately the exclusion of the bad channels during further analyses.\n",
    "HP_TEMP = 1                   \n",
    "USE_RANSAC = True\n",
    "OUT_CSV = \"/Users/lucas.assen/Desktop/Master Thesis/bad_channel_summary.csv\"\n",
    "\n",
    "def scan_raw(raw: mne.io.BaseRaw, hp_temp=HP_TEMP) -> dict:\n",
    "    raw_tmp = raw.copy()\n",
    "    if hp_temp is not None:\n",
    "        raw_tmp.filter(l_freq=hp_temp, h_freq=None, picks=\"all\", verbose=False)\n",
    "\n",
    "    nc = NoisyChannels(raw_tmp, random_state=97, do_detrend=False)\n",
    "    nc.find_all_bads(ransac=USE_RANSAC)\n",
    "\n",
    "    bad_dict = nc.get_bads(as_dict=True)     \n",
    "    stats = {k: len(v) for k, v in bad_dict.items()}\n",
    "    stats.update(\n",
    "        all_bad=len(nc.get_bads()),\n",
    "        bad_ch_names=\",\".join(nc.get_bads()),\n",
    "        n_total=len(raw.ch_names),)\n",
    "    return stats\n",
    "\n",
    "# The scan_raw function is iterated over all participants and the output is stored in csv. \n",
    "# A subsection of the output is printed in order to conirm whether it is correct. \n",
    "\n",
    "rows = []\n",
    "for group, subdict in D_0.items():\n",
    "    print(f\"\\n=== {group} ===\")\n",
    "    for pid, raw in subdict.items():\n",
    "        print(f\"→ {pid}\", end=\"  \")\n",
    "        try:\n",
    "            row = scan_raw(raw)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"{pid} failed: {e}\")\n",
    "            continue\n",
    "        rows.append({\"participant_id\": pid, \"group\": group, **row})\n",
    "        print(f\"bad={row['all_bad']} / {row['n_total']}\")\n",
    "\n",
    "summary = pd.DataFrame(rows).sort_values([\"group\", \"participant_id\"])\n",
    "summary.to_csv(OUT_CSV, index=False)\n",
    "summary.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
