{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timefrequency analysis \n",
    "## 1. Calculation of the ERDS values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re \n",
    "import json \n",
    "import glob\n",
    "import h5py \n",
    "import warnings\n",
    "import numpy as np\n",
    "import mne\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import pandas as pd\n",
    "\n",
    "# The relevant directories are identified here and checked for existence.\n",
    "# These should be changed according to your own file structure.\n",
    "EPO_DIR = \"/Users/lucas.assen/Desktop/Master Thesis/sub_epoch_files\"\n",
    "OUT_DIR = \"/Users/lucas.assen/Desktop/Master Thesis/Time_frequency_analysis_3\"\n",
    "BANDS_CSV = \"/Users/lucas.assen/Desktop/Master Thesis/Data/my_mne_project/mne_env/IAF_After_channel_rejection.csv\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# We created these constants which can be adjusted if wanted. Decimate is 1 refering to no decimation of the EEG signal. \n",
    "# N_JOBS is the number of cpu cores used for paralell processing, this should be tweaked based on what your own system can handle.\n",
    "# BASE_EPS is a small value to avoid log of zero or negative values, during baseline correction.\n",
    "DECIM = 1\n",
    "N_JOBS = 12\n",
    "BASE_EPS = 1e-15          \n",
    "\n",
    "# Similar to previous analysis steps wwe use a helper to normilise the participant IDs across the diffferent files.\n",
    "def norm_pid(v):\n",
    "    m = re.match(r\"^0*?(\\d+)(?:\\\\.0+)?([A-Za-z]*)$\", str(v).strip())\n",
    "    return (m.group(1) + m.group(2)) if m else str(v).strip()\n",
    "\n",
    "# We created a common frequency grid using the following helper function; the grid is from 0.3 to 35 Hz in steps of 0.5 Hz.\n",
    "# Even through individual frequency bands are used for the ERDS masks, a common grid is used first to compute the TFRs.\n",
    "def grid(lo, hi, step=0.5):\n",
    "    return np.arange(lo, hi + step/2, step)\n",
    "\n",
    "COMMON_FREQS = grid(0.3, 35.0, 0.5).round(2)\n",
    "\n",
    "# Next personalised masks based on the individualised frequency bands are created to be used later \n",
    "# to select the bins on a individual basis to calculate ERDS.\n",
    "bands = pd.read_csv(BANDS_CSV)\n",
    "bands.columns = bands.columns.str.strip().str.lower()\n",
    "id_col = next(c for c in bands.columns if \"id\" in c)\n",
    "bands[id_col] = (bands[id_col].astype(str)\n",
    "                 .str.replace(r\"_.*$\", \"\", regex=True)\n",
    "                 .str.strip()\n",
    "                 .apply(norm_pid))\n",
    "\n",
    "indiv_mask_dict = {}\n",
    "for _, row in bands.iterrows():\n",
    "    pid = row[id_col]\n",
    "    mask = np.zeros_like(COMMON_FREQS, dtype=bool)\n",
    "    for lo, hi in [(row.delta_low, row.delta_high),\n",
    "                   (row.theta_low, row.theta_high),\n",
    "                   (row.alpha_low, row.alpha_high),\n",
    "                   (row.beta_low, row.beta_high),]:\n",
    "         mask |= (COMMON_FREQS >= lo) & (COMMON_FREQS <= hi)\n",
    "    indiv_mask_dict[pid] = mask\n",
    "\n",
    "# The regions are again defined and stored in a list to be used for regional averaging of the ERD/S.\n",
    "REGIONS = {\"F1\": [\"AF7\",\"AF3\",\"F7\",\"F5\",\"F3\",\"F1\"],\n",
    "           \"F2\": [\"AF4\",\"AF8\",\"F4\",\"F6\",\"F2\",\"F8\"],\n",
    "           \"C\" : [\"Fz\",\"FC1\",\"FCZ\",\"FC2\",\"C1\",\"Cz\",\"C2\",\"CP1\",\"CPZ\",\"CP2\",\"P1\",\"Pz\",\"P2\",\"POZ\"],\n",
    "           \"O\" : [\"PO7\",\"PO8\",\"PO3\",\"PO4\",\"O1\",\"OZ\",\"O2\"],\n",
    "           \"P1\": [\"TP7\",\"CP5\",\"CP3\",\"P7\",\"P5\",\"P3\"],\n",
    "           \"P2\": [\"CP4\",\"CP6\",\"CP8\",\"P4\",\"P6\",\"P8\"],\n",
    "           \"T1\": [\"FT7\",\"FC5\",\"FC3\",\"T7\",\"C5\",\"C3\"],\n",
    "           \"T2\": [\"FC4\",\"FC6\",\"FC8\",\"C4\",\"C6\",\"T8\"],}\n",
    "REGION_NAMES = list(REGIONS)\n",
    "N_REG = len(REGION_NAMES)\n",
    "\n",
    "# The following loop goes through each participants' extracted epochs file stored in the EPO_DIR path folder.\n",
    "for fn in sorted(os.listdir(EPO_DIR)):\n",
    "    if not fn.endswith(\"_segments-epo.fif\"):\n",
    "        continue\n",
    "    pid = norm_pid(fn.split(\"_\")[0].replace(\"sub-\", \"\"))\n",
    "    if pid not in indiv_mask_dict:\n",
    "        print(f\"participant {pid} skipped as they had no individualised mask\")\n",
    "        continue\n",
    "    \n",
    "    #Participant's their epoch file is loaded and some metadata columns are renamed for consistency.\n",
    "    print(f\"Starting with participant {pid}\")\n",
    "    epochs = mne.read_epochs(os.path.join(EPO_DIR, fn),\n",
    "                             preload=True, verbose=\"ERROR\")\n",
    "    epochs.metadata = epochs.metadata.rename(columns={'block_idx': 'block'})\n",
    "    meta = epochs.metadata.reset_index(drop=True)\n",
    "\n",
    "    # freqs is defined as the eaerlier created common frequency grid and n_cycles is set to half of the frequency value at any frequency \n",
    "    # to balance temporal and spatial precision.\n",
    "    freqs  = COMMON_FREQS\n",
    "    n_cycles = freqs / 2.0\n",
    "\n",
    "    pr_idx = np.where(meta.kind == \"practice\")[0]\n",
    "    m_prep = meta.kind == \"prep\"\n",
    "    if pr_idx.size == 0:\n",
    "        warnings.warn(f\"{pid}: no practice epochs\");  continue\n",
    "\n",
    "    # The regions are mapped based on the channels in the region and the channels present in the current participant's data.\n",
    "    ch_ix = {ch: i for i, ch in enumerate(epochs.ch_names)}\n",
    "    reg_to_ix = [[ch_ix[c] for c in REGIONS[r] if c in ch_ix] for r in REGION_NAMES]\n",
    "\n",
    "    # A dummy TFR is run to get the shape of the data cube in order to create the shape of the H5 dataset.\n",
    "    n_t = epochs[pr_idx[0]:pr_idx[0]+1].compute_tfr(\"morlet\", \n",
    "                                                    freqs=freqs,\n",
    "                                                    n_cycles=n_cycles,\n",
    "                                                    output=\"power\", \n",
    "                                                    decim=DECIM, n_jobs=1).data.shape[-1]\n",
    "\n",
    "    h5path = os.path.join(OUT_DIR, f\"sub-{pid}_tfr_9d.h5\")\n",
    "    with h5py.File(h5path, \"w\") as h5:\n",
    "        ds = h5.create_dataset(\"erds\", (len(pr_idx), 8, len(freqs), n_t),\n",
    "                               dtype=\"float32\", compression=\"gzip\",\n",
    "                               chunks=(1, 8, len(freqs), n_t))\n",
    "        h5.create_dataset(\"freqs\", data=freqs)\n",
    "        h5.create_dataset(\"times\", data=np.arange(n_t)*DECIM/epochs.info[\"sfreq\"])\n",
    "        h5.create_dataset(\"indiv_mask\", data=indiv_mask_dict[pid])\n",
    "        h5.create_dataset(\"region_names\",\n",
    "                          data=np.array(REGION_NAMES, dtype=h5py.string_dtype()))\n",
    "        h5.attrs.update(event_id=json.dumps(epochs.event_id),\n",
    "                        sfreq=epochs.info[\"sfreq\"],\n",
    "                        metric=\"dB_ERDS\")\n",
    "        \n",
    "        # The metadata from the epoch fif files is is also stored in a subgroup in the H5 file.\n",
    "        g_meta = h5.create_group(\"metadata\")\n",
    "        meta_pr = meta.iloc[pr_idx].reset_index(drop=True)\n",
    "        str_dt = h5py.string_dtype(encoding=\"utf-8\")\n",
    "        for col in meta_pr.columns:\n",
    "            arr = meta_pr[col].to_numpy()\n",
    "            if arr.dtype.kind in (\"O\", \"U\", \"S\"):          \n",
    "                data = np.array(arr.astype(str), dtype=object)\n",
    "                g_meta.create_dataset(col, data=data, dtype=str_dt)\n",
    "            else:                                          \n",
    "                g_meta.create_dataset(col, data=arr, dtype=arr.dtype)\n",
    "\n",
    "        # After the creation of the data structure of the H5 file the loop over the practice epochs is started to calculate the ERDS.\n",
    "        # Each practice epoch is seperately processed and written to the H5 file to avoid heavy memory load.\n",
    "        # First the number of prep practice epochs is identified.\n",
    "        for i, ix in enumerate(pr_idx):\n",
    "            blk  = int(meta.block.iloc[ix])\n",
    "            ix_p = np.where(m_prep & (meta.block == blk))[0][0]\n",
    "\n",
    "            # The baseline is calculated by first running the tfr on the prep epoch and then averaging across the duration of the prep phase.\n",
    "            base = epochs[ix_p:ix_p+1].compute_tfr(\"morlet\", freqs=freqs, n_cycles=n_cycles,\n",
    "                                                   output=\"power\", \n",
    "                                                   decim=DECIM, \n",
    "                                                   n_jobs=N_JOBS).data[0]                                 \n",
    "            base_reg = np.stack([base[ix_list].mean(axis=0).mean(axis=-1) if ix_list else np.nan\n",
    "                                 for ix_list in reg_to_ix], axis=0)[:, :, None]                 \n",
    "            base_reg = np.where(base_reg < BASE_EPS, np.nan, base_reg)\n",
    "\n",
    "            # Next the power during the practice epoch is calculated in a similar manner, except no averaging over time is done here.\n",
    "            pow_p = epochs[ix:ix+1].compute_tfr(\"morlet\", freqs=freqs, \n",
    "            n_cycles=n_cycles, \n",
    "            output=\"power\", \n",
    "            decim=DECIM, \n",
    "            n_jobs=N_JOBS).data[0]                                   \n",
    "            pow_reg = np.stack([pow_p[ix_list].mean(axis=0) if ix_list else np.nan\n",
    "                for ix_list in reg_to_ix], axis=0)\n",
    "            # The practice power is then baseline corrected using the earlier calculated baseline from the prep phase of the same block.\n",
    "            ratio  = pow_reg / base_reg  \n",
    "            #Both options are included here for calculating ERDS in either dB or as a percentage. We chose dB in our analysis but you can switch to percentage if wanted.                       \n",
    "            #ds[i] = 100 * (ratio - 1)            \n",
    "            ds[i] = 10.0 * np.log10(ratio)     \n",
    "            del base, base_reg, pow_p, pow_reg\n",
    "            gc.collect()\n",
    "\n",
    "    print(\"saved the data to the H5 file\", h5path)\n",
    "    del epochs; gc.collect()\n",
    "\n",
    "print(\"Done with all participants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extraction of the theta and alpha band ERD/S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following the calculation of the TFR we take the H5 files and transform the data to the final ERD/S output used for further analyses.\n",
    "# We first define the relevant directeries, please change the paths to your own, also make sure that the code uses the same naming conventions as your files.\n",
    "TFR_DIR   = \"/Users/lucas.assen/Desktop/Master Thesis/Time_frequency_analysis_2\"\n",
    "BANDS_CSV = \"/Users/lucas.assen/Desktop/Master Thesis/Data/my_mne_project/mne_env/Individualised_freq_bands_after_chan_reject.csv\"\n",
    "OUT_DIR   = \"/Users/lucas.assen/Desktop/Master Thesis/ERDS_band_csv\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# We again use the helper to normalise participant names, in this case, between the TFR_DIR and the BANDS_CSV\n",
    "def norm_pid(v: str):\n",
    "    m = re.match(r\"^0*?(\\d+)(?:\\\\.0+)?([A-Za-z]*)$\", v.strip())\n",
    "    return (m.group(1) + m.group(2)) if m else v.strip()\n",
    "\n",
    "# The decode helper is needed to turn the meta data, which was attached in the H5 file as a subgroup, back into readable strings instead of binary data.\n",
    "def decode(col):\n",
    "    s = pd.Series(col)\n",
    "    return s.apply(lambda x: x.decode() if isinstance(x, (bytes, bytearray)) else str(x)).str.strip()\n",
    "\n",
    "# We recreated the dictionary for individualised band edges from the BANDS_CSV file for each participant.\n",
    "bands_df = pd.read_csv(BANDS_CSV)\n",
    "bands_df.columns = bands_df.columns.str.lower().str.strip()\n",
    "id_col = next(c for c in bands_df.columns if \"id\" in c)\n",
    "bands_df[id_col] = bands_df[id_col].astype(str).str.replace(r\"_.*$\", \"\", regex=True).apply(norm_pid)\n",
    "\n",
    "band_edges = {}   \n",
    "for _, row in bands_df.iterrows():\n",
    "    band_edges[row[id_col]] = {\"delta\": (row.delta_low,  row.delta_high),\n",
    "    \"theta\": (row.theta_low,  row.theta_high),\n",
    "    \"alpha\": (row.alpha_low,  row.alpha_high),\n",
    "    \"beta\" : (row.beta_low,   row.beta_high),}\n",
    "\n",
    "# Afterwards, we looped through all H5 files in the TFR_DIR. Again make sure to change the \"sub-*_tfr_9d.h5\" in case you use differing naming conventions.\n",
    "for fp in sorted(glob.glob(os.path.join(TFR_DIR, \"sub-*_tfr_9d.h5\"))):\n",
    "    pid = norm_pid(os.path.basename(fp).split(\"_\")[0].replace(\"sub-\", \"\"))\n",
    "    \n",
    "    #A skip is put in place in case the individualised band edges of a participant do not exist, if you have the correct data this should not trigger.\n",
    "    if pid not in band_edges:\n",
    "        warnings.warn(f\"No band limits for {pid}, therefore this participant is skipped.\")\n",
    "        continue\n",
    "    \n",
    "    # The file is loaded and the tree is walked, extracting the erd/s for all frequencies, time points and regions per epoch\n",
    "    print(f\"Currently working on {pid}\")\n",
    "    with h5py.File(fp, \"r\") as h5:\n",
    "        erds = h5[\"erds\"][:]                        # (epochs, 8, F, T)\n",
    "        freqs = h5[\"freqs\"][:]                       # (F,)\n",
    "        times = h5[\"times\"][:]                       # (T,)\n",
    "        region_names = [r.decode() if isinstance(r, (bytes, bytearray)) else str(r)\n",
    "        for r in h5[\"region_names\"][:]]\n",
    "        meta = {k: h5[\"metadata\"][k][:] for k in h5[\"metadata\"]}\n",
    "        meta = pd.DataFrame(meta)\n",
    "\n",
    "    # The meta data is decoded to get the group practice, type and block numbers out for further \n",
    "    meta[\"group\"] = decode(meta[\"group\"]).str.rstrip(\"s\").str.title()\n",
    "    meta[\"practice_type\"] = decode(meta[\"cond\"]).str.upper()\n",
    "    meta[\"block\"] = meta[\"block\"].astype(int)\n",
    "\n",
    "    # The individualised frequency mask is created based on the individualised frequency bands in order \n",
    "    # to select the right frequency bins for each band for the current participant.\n",
    "    masks = {band: (freqs >= lo) & (freqs <= hi)\n",
    "             for band, (lo, hi) in band_edges[pid].items()}\n",
    "\n",
    "    # Bins for the time points are created in order to create 14 equally sized 5 second periods.\n",
    "    t_bins = np.arange(0, 70 + 5, 5)\n",
    "    t_idx  = np.digitize(times, t_bins) - 1\n",
    "\n",
    "    # A list is prepared which is then saved to a csv data frame. Then for each time point bin in each individualised freq band for each region within each block, \n",
    "    # the average ERD/S is calculated and store in a row in the list with the adhering metadata information, i.e., part id, group, block, pract_type, Strecth_k, etc.\n",
    "    rows = []\n",
    "    for ep, row_meta in meta.iterrows():\n",
    "        for r, reg_name in enumerate(region_names):\n",
    "            reg_mat = erds[ep, r]                    \n",
    "            for band, msk in masks.items():\n",
    "                band_pow = np.nanmean(reg_mat[msk, :], axis=0)  \n",
    "                for tp in range(14):                              \n",
    "                    seg_val = np.nanmean(band_pow[t_idx == tp])\n",
    "                    rows.append({\"participant_id\": pid,\n",
    "                    \"group\": row_meta[\"group\"],\n",
    "                    \"block\": row_meta[\"block\"],\n",
    "                    \"practice_type\": row_meta[\"practice_type\"],\n",
    "                    \"stretch_k\": row_meta[\"stretch_k\"],\n",
    "                    \"rating\": row_meta[\"rating\"],\n",
    "                    \"region\": reg_name,\n",
    "                    \"freq_band\": band,\n",
    "                    \"timepoint\": tp + 1,    \n",
    "                    \"erds_db\": seg_val,})\n",
    "    df_out = pd.DataFrame(rows)\n",
    "    out_path = os.path.join(OUT_DIR, f\"sub-{pid}_band_region_timeseries.csv\")\n",
    "    df_out.to_csv(out_path, index=False)\n",
    "    print(f\"Done with {pid}\")\n",
    "print(\"Done with all participants\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The creation of ERD/S maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next to the extraction of the ERD/S values for the individualised band, we also use the H5 files to create general visualisations of the TFR for each region and condition\n",
    "# Make sure your file paths are correct\n",
    "TFR_DIR = \"/Users/lucas.assen/Desktop/Master Thesis/Time_frequency_analysis_2\"\n",
    "OUT_DIR = \"/Users/lucas.assen/Desktop/Master Thesis/Grand_ERDS_maps_2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "#These cells are used to give titles to the plots\n",
    "CELLS = {(\"Buddhist\", \"Fa\"): \"Buddhists_FA\",\n",
    "(\"Buddhist\", \"Lk\"): \"Buddhists_LKM\",\n",
    "(\"Control\",  \"Lk\"): \"Novices_LKM\",}\n",
    "\n",
    "# The freq_max constant is set to 15Hz as the primary interest are the theta and alpha band, thus only plotting the relevant part of the freq frange. \n",
    "FREQ_MAX = 15.0   \n",
    "\n",
    "# The same decoder is needed here to help extract the metadata from the subgroup in the H5 file. \n",
    "def decode_series(s: pd.Series) -> pd.Series:\n",
    "    return (s.apply(lambda x: x.decode() if isinstance(x, (bytes, bytearray)) else str(x))\n",
    "              .str.strip().str.rstrip(\"s\").str.title())\n",
    "\n",
    "# The relevant files are located and for each file the relevant information is retrieved, \n",
    "# i.e, ERD/s per freq, time point and epoch and the group and condition from the metadata.  \n",
    "files = sorted(glob.glob(os.path.join(TFR_DIR, \"sub-*_tfr_9d.h5\")))\n",
    "if not files:\n",
    "    raise RuntimeError(\"No *_tfr_9d.h5 files found\")\n",
    "\n",
    "cell_data, cell_times = {lab: [] for lab in CELLS.values()}, {}\n",
    "for f in files:\n",
    "    with h5py.File(f, \"r\") as h5:\n",
    "        erds = h5[\"erds\"][:]                       \n",
    "        freqs = h5[\"freqs\"][:]\n",
    "        times = h5[\"times\"][:]\n",
    "        meta = pd.DataFrame({k: h5[\"metadata\"][k][:] for k in h5[\"metadata\"]})\n",
    "        grp = decode_series(meta[\"group\"])\n",
    "        cond = decode_series(meta[\"cond\"])\n",
    "\n",
    "        #All epochs belioging to the same group/condition are stacked and collapsed together.\n",
    "        for (g, c), label in CELLS.items():\n",
    "            m = (grp == g) & (cond == c)\n",
    "            if m.any():\n",
    "                cell_data[label].append(erds[m].mean(axis=0))   \n",
    "                cell_times.setdefault(label, times)\n",
    "                if not np.allclose(times, cell_times[label]):\n",
    "                    warnings.warn(f\"time mismatch in {f}\")\n",
    "\n",
    "# When averaging over epoch a grand average is created by averaging over all participants belonging in the same condition for each time point and frequency\n",
    "grand = {lab: np.stack(lst).mean(axis=0) for lab, lst in cell_data.items() if lst}\n",
    "if not grand:\n",
    "    raise RuntimeError(\"No grandaverages calculated\")\n",
    "\n",
    "# Afterwards, regional averages across participants for each group and condition are also calculated.\n",
    "with h5py.File(files[0], \"r\") as h5:\n",
    "    region_names = [r.decode() if isinstance(r, bytes) else r for r in h5[\"region_names\"][:]]\n",
    "\n",
    "freq_mask = freqs <= FREQ_MAX\n",
    "freqs_sub = freqs[freq_mask]\n",
    "\n",
    "# Using the following helper function the grand averages and regional averages for each condition and group are plotted and saved as PNGs to the output directory.\n",
    "def plot_map(Z_full, freqs, times, title, path):\n",
    "    freq_mask = freqs <= FREQ_MAX\n",
    "    freqs_sub = freqs[freq_mask]\n",
    "    Z = Z_full[freq_mask]  \n",
    "    lo, hi = np.nanpercentile(Z, [2, 98])\n",
    "    lim = max(abs(lo), abs(hi))\n",
    "    norm = TwoSlopeNorm(vmin=-lim, vcenter=0, vmax=lim)\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    pcm = plt.pcolormesh(times, freqs_sub, Z, shading=\"auto\", cmap=\"RdBu_r\", norm=norm)\n",
    "    plt.axvline(0, color=\"k\", lw=0.7)\n",
    "    plt.xlabel(\"Time [s]\"); \n",
    "    plt.ylabel(\"Frequency [Hz]\")\n",
    "    plt.title(title)\n",
    "    plt.colorbar(pcm, label=\"ERDS [dB]\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(path, dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "# Next, the plot_map helper function is looped to create the grand average and regional plots for each condition and group.\n",
    "for lab, tensor in grand.items():           \n",
    "    times = cell_times[lab]\n",
    "    for reg_idx, reg_name in enumerate(region_names):\n",
    "        fn = os.path.join(OUT_DIR, f\"{lab}_{reg_name}.png\")\n",
    "        plot_map(tensor[reg_idx], freqs, times, f\"{lab} · {reg_name}\", fn)\n",
    "    all_regions = tensor.mean(axis=0)          \n",
    "    fn_all = os.path.join(OUT_DIR, f\"{lab}_AllRegions.png\")\n",
    "    plot_map(all_regions, freqs, times, f\"{lab} · All Regions\", fn_all)\n",
    "\n",
    "print(\"The ERD/S plots are saved as PNGs to:\", OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The extraction of the alpha/theta ratio (ATR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The relevant directories and constants are similar as before.\n",
    "# Now we do not need a baseline as we want the raw power ratios of alpha and theta, we also can directly save the results to CSV files \n",
    "# as we do not need the full TFR data, only the ratios per region, timepoint and block.\n",
    "EPO_DIR = \"/Users/lucas.assen/Desktop/Master Thesis/sub_epoch_files\"\n",
    "OUT_DIR = \"/Users/lucas.assen/Desktop/Master Thesis/Time_frequency_analysis\"\n",
    "BANDS_CSV = \"/Users/lucas.assen/Desktop/Master Thesis/Data/my_mne_project/mne_env/IAF_After_channel_rejection.csv\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "DECIM = 1\n",
    "N_JOBS = 12\n",
    "\n",
    "# Again a helper to normilise participant IDs is created and the same normalised frequency grid is used for the tfr computations.\n",
    "def norm_pid(v):\n",
    "    m = re.match(r\"^0*?(\\d+)(?:\\\\.0+)?([A-Za-z]*)$\", str(v).strip())\n",
    "    return (m.group(1) + m.group(2)) if m else str(v).strip()\n",
    "\n",
    "def grid(lo, hi, step=0.5):\n",
    "    return np.arange(lo, hi + step/2, step)\n",
    "COMMON_FREQS = grid(0.3, 35.0, 0.5).round(2)\n",
    "\n",
    "# Similar indiv masks are created based on the individualised frequency bands to be used after the tfr computations.\n",
    "bands = pd.read_csv(BANDS_CSV)\n",
    "bands.columns = bands.columns.str.strip().str.lower()\n",
    "id_col = next(c for c in bands.columns if \"id\" in c)\n",
    "bands[id_col] = (bands[id_col].astype(str)\n",
    "                 .str.replace(r\"_.*$\", \"\", regex=True)\n",
    "                 .str.strip()\n",
    "                 .apply(norm_pid))\n",
    "\n",
    "indiv_mask_dict = {}\n",
    "for _, row in bands.iterrows():\n",
    "    pid = row[id_col]\n",
    "    mask = np.zeros_like(COMMON_FREQS, dtype=bool)\n",
    "    for lo, hi in [(row.delta_low, row.delta_high),\n",
    "    (row.theta_low, row.theta_high),\n",
    "    (row.alpha_low, row.alpha_high),\n",
    "    (row.beta_low, row.beta_high),]: \n",
    "        mask |= (COMMON_FREQS >= lo) & (COMMON_FREQS <= hi)\n",
    "    indiv_mask_dict[pid] = mask\n",
    "\n",
    "# The refions are again defined and stored in a list to be used for regional averaging.\n",
    "REGIONS = {\"F1\": [\"AF7\",\"AF3\",\"F7\",\"F5\",\"F3\",\"F1\"],\n",
    "           \"F2\": [\"AF4\",\"AF8\",\"F4\",\"F6\",\"F2\",\"F8\"],\n",
    "           \"C\" : [\"Fz\",\"FC1\",\"FCZ\",\"FC2\",\"C1\",\"Cz\",\"C2\",\"CP1\",\"CPZ\",\"CP2\",\"P1\",\"Pz\",\"P2\",\"POZ\"],\n",
    "           \"O\" : [\"PO7\",\"PO8\",\"PO3\",\"PO4\",\"O1\",\"OZ\",\"O2\"],\n",
    "           \"P1\": [\"TP7\",\"CP5\",\"CP3\",\"P7\",\"P5\",\"P3\"],\n",
    "           \"P2\": [\"CP4\",\"CP6\",\"CP8\",\"P4\",\"P6\",\"P8\"],\n",
    "           \"T1\": [\"FT7\",\"FC5\",\"FC3\",\"T7\",\"C5\",\"C3\"],\n",
    "           \"T2\": [\"FC4\",\"FC6\",\"FC8\",\"C4\",\"C6\",\"T8\"],}\n",
    "\n",
    "REGION_NAMES = list(REGIONS)\n",
    "N_REG = len(REGION_NAMES)\n",
    "\n",
    "CSV_DIR = os.path.join(OUT_DIR, \"alpha_theta_ratio_csv\")\n",
    "os.makedirs(CSV_DIR, exist_ok=True)\n",
    "\n",
    "# The following loop goes through each participants' extracted epochs file stored in the EPO_DIR path folder.\n",
    "for fn in sorted(os.listdir(EPO_DIR)):\n",
    "    if not fn.endswith(\"_segments-epo.fif\"):\n",
    "        continue\n",
    "\n",
    "    pid = norm_pid(fn.split(\"_\")[0].replace(\"sub-\", \"\"))\n",
    "    if pid not in indiv_mask_dict:\n",
    "        print(f\"Participant {pid} was skipped as they had no indiv mask\");  continue\n",
    "    \n",
    "    # Reading the epochs of the current participant and taking the metadata. \n",
    "    print(f\"Currently working on subject {pid}\")\n",
    "    epochs = mne.read_epochs(os.path.join(EPO_DIR, fn),\n",
    "                             preload=True, verbose=\"ERROR\")\n",
    "    epochs.metadata = epochs.metadata.rename(columns={\"block_idx\": \"block\"})\n",
    "    meta = epochs.metadata.reset_index(drop=True)\n",
    "\n",
    "    # The individualised alpha and theta masks are created based on the individualised frequency bands.\n",
    "    row_bands = bands.loc[bands[id_col] == pid].squeeze()\n",
    "    alpha_mask = (COMMON_FREQS >= row_bands.alpha_low) & (COMMON_FREQS <= row_bands.alpha_high)\n",
    "    theta_mask = (COMMON_FREQS >= row_bands.theta_low) & (COMMON_FREQS <= row_bands.theta_high)\n",
    "\n",
    "    # Freqs and cycles are defined and the practice epochs are identified.\n",
    "    freqs = COMMON_FREQS\n",
    "    n_cycles = freqs / 2.0\n",
    "    pr_idx = np.where(meta.kind == \"practice\")[0]\n",
    "\n",
    "    # The regions are defined based on the maps and the available channels in the current participant's data.\n",
    "    ch_ix      = {ch: i for i, ch in enumerate(epochs.ch_names)}\n",
    "    reg_to_ix  = [[ch_ix[c] for c in REGIONS[r] if c in ch_ix] for r in REGION_NAMES]\n",
    "\n",
    "    # A list to store the rows of the CSV file is created.\n",
    "    csv_rows = []\n",
    "\n",
    "    # This loop will go over the participants practice epochs to calculate the alpha/theta ratio per region and timepoint and save it to the CSV file.\n",
    "    for ix in pr_idx:\n",
    "        blk = int(meta.block.iloc[ix])\n",
    "\n",
    "        # The TFR is run on the practice epoch to get the power data.\n",
    "        tfr_p = epochs[ix:ix+1].compute_tfr(\"morlet\", \n",
    "                                            freqs=freqs, \n",
    "                                            n_cycles=n_cycles,\n",
    "                                            output=\"power\", \n",
    "                                            decim=DECIM, \n",
    "                                            n_jobs=N_JOBS)\n",
    "        pow_p   = tfr_p.data[0]                   \n",
    "        times   = tfr_p.times - tfr_p.times[0]    \n",
    "\n",
    "        # The power is averaged for each identified region.\n",
    "        pow_reg = np.stack([\n",
    "            pow_p[ix_list].mean(axis=0) if ix_list else np.nan\n",
    "            for ix_list in reg_to_ix\n",
    "        ], axis=0)\n",
    "\n",
    "        # The power in each region is averaged across 14 time bins of 5 seconds each for each frequency.\n",
    "        for tp in range(14):\n",
    "            t0, t1 = 5 * tp, 5 * (tp + 1)\n",
    "            t_sel = np.where((times >= t0) & (times < t1))[0]\n",
    "            if t_sel.size == 0:\n",
    "                continue\n",
    "\n",
    "            # Then in each region and time bin the ATR is calculated by taking the mean power in \n",
    "            # the individualised alpha band divided by the mean power in the individualised theta band.\n",
    "            for r_i, r_name in enumerate(REGION_NAMES):\n",
    "                alpha_pow = pow_reg[r_i, alpha_mask][:, t_sel].mean()\n",
    "                theta_pow = pow_reg[r_i, theta_mask][:, t_sel].mean()\n",
    "                ratio = alpha_pow / theta_pow if theta_pow > 0 else np.nan\n",
    "\n",
    "                # Lastly, the results and the metadata are appended to the list.\n",
    "                csv_rows.append({\"participant_id\" : pid,\n",
    "                                 \"group\" : meta.group.iloc[ix],\n",
    "                                 \"block\" : blk,\n",
    "                                 \"timepoint\" : tp + 1,\n",
    "                                 \"rating\" : meta.rating.iloc[ix],\n",
    "                                 \"region\": r_name,\n",
    "                                 \"alpha_theta_ratio\" : ratio})\n",
    "        # After writing the data to the list the tfr of the epoch is removed from memory to avoid overload.\n",
    "        del tfr_p, pow_p, pow_reg\n",
    "        gc.collect()\n",
    "\n",
    "    #Lastly, a csv file is created from the complete list of the current participant.\n",
    "    df_csv = pd.DataFrame(csv_rows)\n",
    "    csv_path = os.path.join(CSV_DIR, f\"sub-{pid}_alpha_theta_ratio.csv\")\n",
    "    df_csv.to_csv(csv_path, index=False)\n",
    "    print(\"saved to\", csv_path)\n",
    "    del epochs\n",
    "    gc.collect()\n",
    "\n",
    "print(\"All participants are done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
