{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracton of the preparatory and practice blocks from the preprocessed fif files\n",
    "\n",
    "## 1. Importing libraries, data and removing bad channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import re          \n",
    "import gc\n",
    "from scipy import interpolate\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Similar to the IAF extraction script, we load the csv containing the bad channels.\n",
    "BAD_CSV = \"/Users/lucas.assen/Desktop/Master Thesis/bad_channel_summary_2.csv\"\n",
    "bad_df = pd.read_csv(BAD_CSV,\n",
    "                     sep=\";\",\n",
    "                     engine=\"python\",\n",
    "                     quoting=csv.QUOTE_NONE).fillna(\"\")\n",
    "\n",
    "bad_df.columns = bad_df.columns.str.strip().str.lower()\n",
    "\n",
    "# Again participant keys are created to line up the naming conventions of the partcpants included in the BAD_CSV and to the fif files.\n",
    "bad_map = {}\n",
    "for _, row in bad_df.iterrows():\n",
    "    base = re.match(r\"^0*?(\\d+)\", str(row[\"participant_id\"])).group(1)\n",
    "    suffix = \"c\" if str(row.get(\"group\", \"\")).strip().lower().startswith(\"control\") else \"\"\n",
    "    key = f\"{base}{suffix}\"          \n",
    "    bads = [ch for ch in str(row[\"bad_ch_names\"]).split(\",\") if ch]\n",
    "    bad_map[key] = bads\n",
    "\n",
    "\n",
    "#Locating the preprocessed fif files and storing them in a list of fles\n",
    "#Adapt the paths to your own file structure if needed.\n",
    "buddhist_dir = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Buddhists_preprocessed\")\n",
    "control_dir = Path(\"/Users/lucas.assen/Desktop/Master Thesis/Controls_preprocessed\")\n",
    "\n",
    "buddhist_fif_files = sorted(glob.glob(str(buddhist_dir / \"*.fif\")))\n",
    "control_fif_files = sorted(glob.glob(str(control_dir / \"*.fif\")))\n",
    "\n",
    "\n",
    "#Normalisng participant IDS to match them with the earlier created bad_map keys.\n",
    "def norm_pid(pid_raw: str, condition: str) -> str:\n",
    "    base = re.match(r\"^0*?(\\d+)\", str(pid_raw)).group(1)\n",
    "    suffix = \"c\" if condition.lower().startswith(\"control\") else \"\"\n",
    "    return f\"{base}{suffix}\"\n",
    "\n",
    "\n",
    "#All files are loaded into a dictionary and in cases that a partcipant is loaded that according to the bad_map, has bad channels, has them removed.\n",
    "D_0 = {\"Buddhists\": {}, \"Controls\": {}}\n",
    "\n",
    "def load_data_files(file_list, condition):\n",
    "    for file in file_list:\n",
    "        pid_raw = Path(file).stem.split(\"_\")[0]      \n",
    "        pid = norm_pid(pid_raw, condition)               \n",
    "        print(f\"Currently loading {pid} ({condition})\")\n",
    "\n",
    "        raw = mne.io.read_raw_fif(file, preload=True, verbose=\"ERROR\")\n",
    "\n",
    "        bads = [ch for ch in bad_map.get(pid, []) if ch in raw.ch_names]\n",
    "        if bads:\n",
    "            print(f\"{len(bads)} bad channels dropped: {bads}\")\n",
    "            raw.drop_channels(bads)\n",
    "\n",
    "        D_0[condition][f\"{pid}_RAW\"] = raw\n",
    "\n",
    "load_data_files(buddhist_fif_files, \"Buddhists\")\n",
    "load_data_files(control_fif_files, \"Controls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Correction of the event markers & extraction of the practice and preparatory blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#An additional correction table is loaded for corrections on the block event markers as some of the blocks have incorrect markers.\n",
    "#These are directly updated based on the correct markers found in the logfiles of the experiment,\n",
    "CSV = \"/Users/lucas.assen/Desktop/Master Thesis/block_corrections_final_2.csv\"\n",
    "\n",
    "#We created a corrections map wherby again the participant IDs are again normalised to match the keys of the fif files.\n",
    "corr = (pd.read_csv(CSV).assign(group=lambda d: d.group.str.strip().replace({\"Buddhist\": \"Buddhists\", \"Control\" : \"Controls\"})))\n",
    "\n",
    "def norm_pid(v: str) -> str:\n",
    "    m = re.match(r\"^0*?(\\d+)(?:\\.\\d+)?([A-Za-z]*)$\", str(v).strip())\n",
    "    return (m.group(1) + m.group(2)) if m else str(v).strip()\n",
    "\n",
    "corr[\"participant_id\"] = corr[\"participant_id\"].apply(norm_pid)\n",
    "corr[\"block_idx\"] = corr[\"block_idx\"].astype(int)\n",
    "corr_map = {(r.group, r.participant_id, r.block_idx): r.true_cond\n",
    "            for r in corr.itertuples(index=False)}\n",
    "\n",
    "#The following helper function allows for the strecthing of practice blocks shorter than 70 seconds to exactly 70 seconds using 1D linear interpolation.\n",
    "#Blocks equal to or longer than 70 seconds return a raw.copy.\n",
    "def stretch_raw_uniform(raw: mne.io.BaseRaw, target_sec=70.0):\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    tgt = int(round(target_sec * sf))\n",
    "    data = raw.get_data()\n",
    "    n_ch, n_old = data.shape\n",
    "    if n_old == tgt:\n",
    "        return raw.copy()\n",
    "    x_old = np.linspace(0, 1, n_old, endpoint=False)\n",
    "    x_new = np.linspace(0, 1, tgt, endpoint=False)\n",
    "    data2 = np.empty((n_ch, tgt), dtype=data.dtype)\n",
    "    for c in range(n_ch):\n",
    "        data2[c] = np.interp(x_new, x_old, data[c])\n",
    "    return mne.io.RawArray(data2, raw.info.copy())\n",
    "\n",
    "\n",
    "#The following function is used to extract and cut or interpolate the prep and practice blocks.\n",
    "def extract_blocks(raw, group, raw_key):\n",
    "    sf = raw.info[\"sfreq\"]\n",
    "    evt = mne.find_events(raw, \"Status\", initial_event=True, shortest_event=0)\n",
    "\n",
    "    pid = norm_pid(re.match(r\"(\\d+)\", raw_key).group(1) +\n",
    "                   (\"c\" if group == \"Controls\" else \"\"))\n",
    "\n",
    "    #We index the prep and practice blocks in pairs, so that the correct practice block can be matched to its prep block. \n",
    "    recs, pair_idx, i = [], 0, 0\n",
    "    while i < len(evt):\n",
    "        code = evt[i, 2]\n",
    "\n",
    "        #Here the prep phases are extracted using their originial event markers, any mismatches with the corr_map are  corrected later.\n",
    "        if code in (10, 11):\n",
    "            pair_idx += 1\n",
    "            blk_idx = pair_idx\n",
    "            cond0 = \"FA\" if code == 10 else \"LKM\"\n",
    "            expect = 99 if cond0 == \"FA\" else 100\n",
    "            t0 = evt[i, 0] / sf\n",
    "            j = i + 1\n",
    "            while j < len(evt) and evt[j, 2] not in (expect, 200):\n",
    "                j += 1\n",
    "            t1 = evt[j, 0] / sf if j < len(evt) else raw.times[-1]\n",
    "\n",
    "            recs.append(dict(kind=\"prep\", group=group, participant_id=pid,\n",
    "                             block_idx=blk_idx, default_cond=cond0,\n",
    "                             rating=None,\n",
    "                             duration=t1 - t0,\n",
    "                             raw_orig = raw.copy().crop(t0, t1),\n",
    "                             raw_70s = None,          \n",
    "                             stretch_k=None))         \n",
    "            i = j + 1 if (cond0 == \"LKM\" and evt[j, 2] == 200) else j\n",
    "            continue\n",
    "\n",
    "        #Here we extract the practice phases in similar fashion to the prep phases and is given the same pair index, which together stores them as a block.\n",
    "        if code in (99, 100):\n",
    "            cond0 = \"FA\" if code == 99 else \"LKM\"\n",
    "            expect_pr = 199 if cond0 == \"FA\" else 200\n",
    "            blk_idx = pair_idx                 \n",
    "            t0 = evt[i, 0] / sf\n",
    "\n",
    "            j = i + 1\n",
    "            while j < len(evt) and evt[j, 2] != expect_pr:\n",
    "                j += 1\n",
    "            t1 = evt[j, 0] / sf if j < len(evt) else raw.times[-1]\n",
    "\n",
    "            # Rating is stored as a seperate variable \n",
    "            k, rating = j + 1, None\n",
    "            while k < len(evt):\n",
    "                nxt = evt[k, 2]\n",
    "                if nxt in (1, 2, 3):\n",
    "                    rating, k = int(nxt), k + 1\n",
    "                    break\n",
    "                if nxt in (10, 11, 99, 100):\n",
    "                    break\n",
    "                k += 1\n",
    "\n",
    "            # The corrected labels from the corr_map are used to update the original block condition labels.\n",
    "            # afterwards, based on our studies exclusion criteria, we dropped blocks that were divine love (DL) or rated as 1.\n",
    "            true_cond = corr_map.get((group, pid, blk_idx), cond0)\n",
    "            if true_cond == \"DL\" or rating in (0, 1, None):\n",
    "                recs = [r for r in recs\n",
    "                        if not (r[\"participant_id\"] == pid and\n",
    "                                r[\"block_idx\"] == blk_idx)]\n",
    "                i = k\n",
    "                continue\n",
    "\n",
    "            # Next the durtions of the practice blocks are determined. For blocks shorter than 70 seconds a stretch factor is calculated and stored as a variable and the block is stretched.\n",
    "            # Blocks longer than 70 seconds are trimmed and the strech factor is set to 1.0.\n",
    "            pr_raw_orig = raw.copy().crop(t0, t1)\n",
    "            dur_orig = t1 - t0\n",
    "            if dur_orig < 70.0:                           \n",
    "                stretch_k = 70.0 / dur_orig\n",
    "                pr_raw_70 = stretch_raw_uniform(pr_raw_orig, 70.0)\n",
    "            else:                                         \n",
    "                stretch_k = 1.0\n",
    "                pr_raw_70 = pr_raw_orig.copy().crop(tmax=70.0, include_tmax=False)\n",
    "\n",
    "            # The prep phases that are paired with practice phases that were stretched are also stretched using the same factor.\n",
    "            # if, as a result of using the same stretch factor the prep phase exceeds 70 seconds, it is trimmed to 70 seconds.\n",
    "            if stretch_k > 1.0:                         \n",
    "                for r in recs:\n",
    "                    if (r[\"kind\"] == \"prep\" and\n",
    "                        r[\"participant_id\"] == pid and\n",
    "                        r[\"block_idx\"] == blk_idx):\n",
    "                        orig_len  = r[\"duration\"]                \n",
    "                        new_len = orig_len * stretch_k        \n",
    "                        \n",
    "                        if new_len > 70.0:                      \n",
    "                            new_len = 70.0\n",
    "\n",
    "                        r[\"raw_70s\"] = stretch_raw_uniform(r[\"raw_orig\"], new_len)\n",
    "                        r[\"stretch_k\"] = stretch_k             \n",
    "                        r[\"duration\"] = new_len               \n",
    "                        break\n",
    "            else:                                         \n",
    "                for r in recs:\n",
    "                    if (r[\"kind\"] == \"prep\" and\n",
    "                        r[\"participant_id\"] == pid and\n",
    "                        r[\"block_idx\"] == blk_idx and\n",
    "                        r[\"raw_70s\"] is None):\n",
    "                        r[\"raw_70s\"] = r[\"raw_orig\"]\n",
    "                        r[\"stretch_k\"] = 1.0\n",
    "                        break\n",
    "            \n",
    "            # Lastly, all variables related to each practice block are stored in a dictionary and appended to a list of records.\n",
    "            recs.append(dict(kind=\"practice\", group=group, participant_id=pid,\n",
    "                             block_idx=blk_idx, default_cond=cond0,\n",
    "                             rating=rating, duration=70.0,\n",
    "                             raw_orig = pr_raw_orig,\n",
    "                             raw_70s = pr_raw_70,\n",
    "                             stretch_k = stretch_k))\n",
    "            i = k\n",
    "            continue\n",
    "        i += 1\n",
    "    return recs\n",
    "\n",
    "# Ultimately, the output of the extraction function is stored per participant in a complete list.\n",
    "all_segments = []\n",
    "\n",
    "# To do so the extraction function is iterated over all participants in the D_0 dictionary.\n",
    "for grp, subs in D_0.items():\n",
    "    for key, raw in subs.items():\n",
    "        for seg in extract_blocks(raw, grp, key):\n",
    "\n",
    "            final_cond = corr_map.get((seg[\"group\"], seg[\"participant_id\"], seg[\"block_idx\"]),seg[\"default_cond\"])\n",
    "\n",
    "            # Here all relevant variables are stored and appended to the complete list.\n",
    "            all_segments.append({\"group\": seg[\"group\"],\n",
    "                                 \"participant_id\": seg[\"participant_id\"],\n",
    "                                 \"block_idx\": seg[\"block_idx\"],\n",
    "                                 \"kind\": seg[\"kind\"],      \n",
    "                                 \"cond\": final_cond,           \n",
    "                                 \"duration_s\": seg[\"duration\"],\n",
    "                                 \"rating\": seg[\"rating\"],\n",
    "                                 \"stretch_k\": seg.get(\"stretch_k\", 1.0), \n",
    "                                 \"raw_orig\": seg[\"raw_orig\"],\n",
    "                                 \"raw_70s\": seg[\"raw_70s\"] or seg[\"raw_orig\"],})\n",
    "\n",
    "#Lastly, we put in an extra check to ensure that all prep blocks have a corresponding practice block.\n",
    "#In case a prep block is on its own it is removed from the complete list.\n",
    "have_practice = {(s[\"participant_id\"], s[\"block_idx\"])\n",
    "                 for s in all_segments\n",
    "                 if s[\"kind\"] == \"practice\"}\n",
    "\n",
    "all_segments = [s for s in all_segments\n",
    "                if not (s[\"kind\"] == \"prep\"\n",
    "                        and (s[\"participant_id\"], s[\"block_idx\"])\n",
    "                           not in have_practice)]\n",
    "                           \n",
    "print(f\"Done, {len(all_segments)} segments extracted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Visually inspecting the extracted blocks to show the dilation effect of the linear interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To display the elongated blocks we first have to identify the blocks that have been stretched, preferably by comparing their original durations.\n",
    "# The following helper function helps recalculate the original duration of a block in seconds.\n",
    "def get_orig_len(seg):\n",
    "    raw_o = seg[\"raw_orig\"]\n",
    "    return raw_o.n_times / raw_o.info[\"sfreq\"]\n",
    "\n",
    "# Next we identify all stretched practice blocks by those whoms original durations are shorter than 70 seconds.\n",
    "# The shortest of these blocks, i.e., the one with the largest stretch factor, is selected for demonstration.\n",
    "# A failsafe is included to raise an error in case no stretched blocks are found, though this should not happen.\n",
    "stretch_cands = [s for s in all_segments\n",
    "                 if s[\"kind\"] == \"practice\" and get_orig_len(s) < 70.0 - 1e-3]\n",
    "\n",
    "if not stretch_cands:\n",
    "    raise RuntimeError(\"No stretched blocks found\")\n",
    "\n",
    "seg_demo = max(stretch_cands,\n",
    "               key=lambda s: 70.0 - get_orig_len(s))\n",
    "raw_orig = seg_demo[\"raw_orig\"]       \n",
    "raw_70s  = seg_demo[\"raw_70s\"]        \n",
    "\n",
    "# Relevant info about the block is stored and printed.\n",
    "info_str = (f\"P{seg_demo['participant_id']}\"\n",
    "            f\"blk {seg_demo['block_idx']}\"\n",
    "            f\"{seg_demo['cond']}  rating={seg_demo['rating']}\")\n",
    "\n",
    "print(f\"Block with largest dilation:\\n\"\n",
    "      f\"Original length: {get_orig_len(seg_demo):.2f} s\\n\"\n",
    "      f\"Length after dilation: 70.00 s\")\n",
    "\n",
    "# In the following two plots we display the original and stretched version of the block that had the highest stretch factor.\n",
    "# This helps for a good side by side comparison of the signal pre and post dilation in the most extreme case.\n",
    "raw_orig.plot(title=f\"{info_str} Original ({get_orig_len(seg_demo):.2f} s)\",       \n",
    "              start=0.0,           \n",
    "              duration=9.0,         \n",
    "              scalings=\"auto\",\n",
    "              butterfly=False,\n",
    "              show=True,\n",
    "              block=False,)\n",
    "\n",
    "raw_70s.plot(title=f\"{info_str} Stretched (70 s, first 9 s shown)\",        \n",
    "             start=0.0,            \n",
    "             duration=9.0,         \n",
    "             scalings=\"auto\",\n",
    "             butterfly=False,\n",
    "             show=True,\n",
    "             block=False,)\n",
    "\n",
    "# Next, we created a overlay of the power spectral densities (PSDs) of both the original and stretched version of the block.\n",
    "fmax_psd = 45.0 \n",
    "psd_o  = raw_orig.compute_psd(fmax=fmax_psd, verbose=False)\n",
    "psd_70 = raw_70s .compute_psd(fmax=fmax_psd, verbose=False)\n",
    "freqs = psd_o.freqs\n",
    "pow_o  = psd_o .get_data().mean(axis=0)   \n",
    "pow_70 = psd_70.get_data().mean(axis=0)\n",
    "fig, ax_p = plt.subplots(figsize=(8, 4))\n",
    "ax_p.plot(freqs, pow_o, label=\"Original\")\n",
    "ax_p.plot(freqs, pow_70, label=\"Stretched\")\n",
    "ax_p.set(xlabel=\"frequency [Hz]\",\n",
    "         ylabel=r\"Power (V$^2$/Hz)\",\n",
    "         title=f\"PSD comparison - {info_str}\",)\n",
    "ax_p.legend(frameon=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After properly extracting all prep and practice blocks D_0 is emptied and removed to free up memory.\n",
    "for condition in list(D_0):\n",
    "    for participant, raw in D_0[condition].items():\n",
    "        raw.close()            \n",
    "    D_0[condition].clear()\n",
    "del D_0\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Saving and checking the extracted blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The file path should be changed into your own file structure.\n",
    "# an event id is created to seperate \n",
    "OUT_DIR = \"/Users/lucas.assen/Desktop/Master Thesis/sub_epoch_files_2\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "event_id = {\"prep\": 1, \"practice\": 2}\n",
    "\n",
    "def raw_to_use(seg):\n",
    "    return seg[\"raw_70s\"]\n",
    "\n",
    "# Each participant is stored in a fif with the included epochs of their prep and practice blocks and the relevant metadata.\n",
    "participants = sorted({s[\"participant_id\"] for s in all_segments})\n",
    "for pid in participants:\n",
    "    segs = [s for s in all_segments if s[\"participant_id\"] == pid]\n",
    "    if not segs:\n",
    "        continue\n",
    "\n",
    "    #The metadata is stored as a pandas df, excluding the raw objects\n",
    "    meta = pd.DataFrame([{k: v for k, v in s.items() if k not in (\"raw_orig\", \"raw_70s\")}\n",
    "                         for s in segs])\n",
    "\n",
    "    #The info of the raw object is extracted to create the MNE info object.\n",
    "    ref_raw = raw_to_use(segs[0])\n",
    "    sfreq = ref_raw.info[\"sfreq\"]\n",
    "    ch_names = ref_raw.info[\"ch_names\"]\n",
    "    ch_types = ref_raw.get_channel_types()\n",
    "\n",
    "    # The longest segment is determined to ensure that the data cube can accomodate all segments.\n",
    "    # This should be 70 seconds as all segments have been trimmed or stretched to this length.\n",
    "    max_len = max(raw_to_use(s).n_times for s in segs)\n",
    "\n",
    "    # The data cube and events array are created.\n",
    "    n_ep = len(segs)\n",
    "    data = np.zeros((n_ep, len(ch_names), max_len), dtype=np.float32)\n",
    "    events = np.zeros((n_ep, 3), dtype=int)\n",
    "\n",
    "    # The epoch are copied into the data events array\n",
    "    for idx, seg in enumerate(segs):\n",
    "        raw = raw_to_use(seg).copy().pick(ch_names).load_data()\n",
    "        n_t = raw.n_times\n",
    "        data[idx, :, :n_t] = raw.get_data()\n",
    "        events[idx]        = [idx, 0, event_id[seg[\"kind\"]]]\n",
    "        raw.close()\n",
    "\n",
    "    # The epochs array is created and savedcombining the event data cube and metadata/info on the epochs.\n",
    "    info   = mne.create_info(ch_names, sfreq, ch_types)\n",
    "    epochs = mne.EpochsArray(data, info,\n",
    "                             events = events,\n",
    "                             event_id  = event_id,\n",
    "                             tmin      = 0.0,\n",
    "                             metadata  = meta)\n",
    "\n",
    "    fout = os.path.join(OUT_DIR, f\"sub-{pid}_segments-epo.fif\")\n",
    "    epochs.save(fout, overwrite=True)\n",
    "    print(f\"{pid}: stored {n_ep} epochs in the fif file\")\n",
    "\n",
    "    # Again memory is cleared after each participant, to make it easier to run.\n",
    "    del epochs, data, events, meta\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After saving the extracted prep and practice epochs we can load a participant's epochs file to inspect the metadata and the epochs themselves. \n",
    "# Change the file path approapriately to point to the partcipant you want to inspect.\n",
    " \n",
    "FILE = Path(\"/Users/lucas.assen/Desktop/Master Thesis/sub_epoch_files\")\n",
    "FILE = FILE / \"sub-1_segments-epo.fif\"       \n",
    "\n",
    "# The file is loaded and a header of the meta data including additional information about the participant and block types is printed.\n",
    "epoch = mne.read_epochs(FILE, preload=False, verbose=\"error\")\n",
    "meta = epoch.metadata          \n",
    "\n",
    "print(f\"\\nFile :  {FILE.name}\")\n",
    "print(f\"sfreq:  {epoch.info['sfreq']} Hz\")\n",
    "print(f\"epochs: {len(epoch)}  |  channels: {len(epoch.ch_names)}\")\n",
    "print(\"Counts (kind x cond):\")\n",
    "print(meta.groupby([\"kind\", \"cond\"]).size().unstack(fill_value=0))\n",
    "print(\"\\nMetadata:\")\n",
    "display(meta.head())\n",
    "\n",
    "# Following the inspection of the metadata, the prep and practice blocks of the selected participant can be plotted to visually confirm that they are correctly extracted.\n",
    "# Currently, the first prep and adhering practce block are plotted, change the indexes if you want to see other blocks.\n",
    "idx_prep     = meta.query(\"kind=='prep'\").index[0]\n",
    "idx_practice = meta.query(\"kind=='practice'\").index[0]\n",
    "print(\"\\nPlot of first prep epoch\")\n",
    "epoch[idx_prep].plot(title=f\"{FILE.stem} - first prep epoch\")\n",
    "print(\"\\nPlot of first practice epoch\")\n",
    "epoch[idx_practice].plot(title=f\"{FILE.stem} - first practice block\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
